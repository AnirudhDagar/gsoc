{
  
    
  
    
        "post1": {
            "title": "Tutorial: TMVA PyTorch Interface",
            "content": "Introduction . This tutorial aims to walkthrough the latest addition in the ROOT TMVA module, The PyTorch Interface! This is specifically designed to utilize the puissance of ROOT for working with high energy physics data, while leveraging the power and flexibility of the popular Machine Learning framework, PyTorch. . The PyTorch interface allows HEP Scientists to be more ingenious with ideas and provides the ability to customize Machine Learning models to a far more preponderant extent than the Keras Interface. . Here, we&#39;ll build a simple classifier in python using PyTorch and compare the performance to a few other methods on a test example root dataset. The same model can be achieved in C/C++ like other TMVA methods which are implemented in C/C++. You can follow this if you prefer a C++ implementation. . Imports . We start with importing the necessary modules required for the tutorial . from ROOT import TMVA, TFile, TTree, TCut from subprocess import call from os.path import isfile import torch from torch import nn . Welcome to JupyROOT 6.23/01 . Setup TMVA . TMVA requires initialization the PyMVA to utilize PyTorch. PyMVA is the interface for third-party MVA tools based on Python. It is created to make powerful external libraries easily accessible with a direct integration into the TMVA workflow. All PyMVA methods provide the same plug-and-play mechanisms as the TMVA methods. Because the base method of PyMVA is inherited from the TMVA base method, all options of internal TMVA methods apply for PyMVA methods as well. . TMVA.Tools.Instance() TMVA.PyMethodBase.PyInitialize() output = TFile.Open(&#39;TMVA.root&#39;, &#39;RECREATE&#39;) factory = TMVA.Factory(&#39;TMVAClassification&#39;, output, &#39;!V:!Silent:Color:DrawProgressBar:&#39; &#39;Transformations=D,G:AnalysisType=Classification&#39;) . DataLoader . Below we start by downloading the dataset example dataset. The signal and background are loaded and read from the dataset. Finally we define our dataloader with options to help us control the split. . if not isfile(&#39;tmva_class_example.root&#39;): call([&#39;curl&#39;, &#39;-O&#39;, &#39;http://root.cern.ch/files/tmva_class_example.root&#39;]) data = TFile.Open(&#39;tmva_class_example.root&#39;) signal = data.Get(&#39;TreeS&#39;) background = data.Get(&#39;TreeB&#39;) dataloader = TMVA.DataLoader(&#39;dataset&#39;) for branch in signal.GetListOfBranches(): dataloader.AddVariable(branch.GetName()) dataloader.AddSignalTree(signal, 1.0) dataloader.AddBackgroundTree(background, 1.0) dataloader.PrepareTrainingAndTestTree(TCut(&#39;&#39;), &#39;nTrain_Signal=4000:&#39; &#39;nTrain_Background=4000:&#39; &#39;SplitMode=Random:&#39; &#39;NormMode=NumEvents:!V&#39;) . DataSetInfo : [dataset] : Added class &#34;Signal&#34; : Add Tree TreeS of type Signal with 6000 events DataSetInfo : [dataset] : Added class &#34;Background&#34; : Add Tree TreeB of type Background with 6000 events : Dataset[dataset] : Class index : 0 name : Signal : Dataset[dataset] : Class index : 1 name : Background . Generate model . Model Definition. . model = nn.Sequential() model.add_module(&#39;linear_1&#39;, nn.Linear(in_features=4, out_features=64)) model.add_module(&#39;relu&#39;, nn.ReLU()) model.add_module(&#39;linear_2&#39;, nn.Linear(in_features=64, out_features=2)) model.add_module(&#39;softmax&#39;, nn.Softmax(dim=1)) . Define loss function and the Optimizer. . loss = torch.nn.MSELoss() optimizer = torch.optim.SGD . Define the train and predict function. Note that the arguments to train and predict function need to be fixed since we call the same method internally in the TMVA interface backend. A user may control the training process and the loop inside. . . Tip: You may implement the training loop using tqdm for a nice progress bar! üòÅ . def train(model, train_loader, val_loader, num_epochs, batch_size, optimizer, criterion, save_best, scheduler): trainer = optimizer(model.parameters(), lr=0.01) schedule, schedulerSteps = scheduler best_val = None for epoch in range(num_epochs): # Training Loop # Set to train mode model.train() running_train_loss = 0.0 running_val_loss = 0.0 for i, (X, y) in enumerate(train_loader): trainer.zero_grad() output = model(X) train_loss = criterion(output, y) train_loss.backward() trainer.step() # print train statistics running_train_loss += train_loss.item() if i % 64 == 63: # print every 64 mini-batches print(f&quot;[Epoch {epoch+1}, {i+1}] train loss:&quot; f&quot;{running_train_loss / 64 :.3f}&quot;) running_train_loss = 0.0 if schedule: schedule(optimizer, epoch, schedulerSteps) # Validation Loop # Set to eval mode model.eval() with torch.no_grad(): for i, (X, y) in enumerate(val_loader): output = model(X) val_loss = criterion(output, y) running_val_loss += val_loss.item() curr_val = running_val_loss / len(val_loader) if save_best: if best_val==None: best_val = curr_val best_val = save_best(model, curr_val, best_val) # print val statistics per epoch print(f&quot;[Epoch {epoch+1}] val loss: {curr_val :.3f}&quot;) running_val_loss = 0.0 print(f&quot;Finished Training on {epoch+1} Epochs!&quot;) return model . Define predict function. . . Important: TMVA&#8217;s interface deals with the dataset by converting them to numpy arrays internally and later serving torch dataloaders which we indeed pass into our training function. But, note that at test/inference time, we are presented with numpy array inputs and we need to convert those to PyTorch tensors within the predict method. Similarly, we need to return the predicted numpy array back to TMVA. . def predict(model, test_X, batch_size=32): # Set to eval mode model.eval() X = torch.Tensor(test_X) with torch.no_grad(): predictions = model(X) return predictions.numpy() . Now that we have defined the necessary components required for downloading, preprocessing, dataloaders, building our model, training loop, and a prediction method for evaluation. We need to share some of these components with the TMVA Interface backend. . . This can be simply done by defining dictionary: load_model_custom_objects . Keys: . &quot;optimizer&quot; | &quot;criterion&quot; | &quot;train_func&quot; | &quot;predict_func&quot; | . . Note: The keys and the name of the dictionary need to be as specified! . # Pass optimizer, loss, train, predict function objects, # defined earlier, as values to the dictionary load_model_custom_objects = {&quot;optimizer&quot;: optimizer, &quot;criterion&quot;: loss, &quot;train_func&quot;: train, &quot;predict_func&quot;: predict} . Save &amp; Store model . Since the TMVA interface requires us to load the model from the stored file without re-definition of model class, we need to convert the model to torchscript before saving for achieving this functionality. . print(model) m = torch.jit.script(model) torch.jit.save(m, &quot;model.pt&quot;) . Sequential( (linear_1): Linear(in_features=4, out_features=64, bias=True) (relu): ReLU() (linear_2): Linear(in_features=64, out_features=2, bias=True) (softmax): Softmax(dim=1) ) . Book TMVA methods . Now we will proceed by booking the MVA methods which we require. We book Fisher and PyTorch methods. . factory.BookMethod(dataloader, TMVA.Types.kFisher, &#39;Fisher&#39;, &#39;!H:!V:Fisher:VarTransform=D,G&#39;) factory.BookMethod(dataloader, TMVA.Types.kPyTorch, &#39;PyTorch&#39;, &#39;H:!V:VarTransform=D,G:FilenameModel=model.pt:&#39; &#39;NumEpochs=20:BatchSize=32&#39;) . custom objects for loading model : {&#39;optimizer&#39;: &lt;class &#39;torch.optim.sgd.SGD&#39;&gt;, &#39;criterion&#39;: MSELoss(), &#39;train_func&#39;: &lt;function train at 0x1307b99e0&gt;, &#39;predict_func&#39;: &lt;function predict at 0x1307b9f80&gt;} . &lt;cppyy.gbl.TMVA.MethodPyTorch object at 0x7f908f837600&gt; . Factory : Booking method: Fisher : Fisher : [dataset] : Create Transformation &#34;D&#34; with events from all classes. : : Transformation, Variable selection : : Input : variable &#39;var1&#39; &lt;&gt; Output : variable &#39;var1&#39; : Input : variable &#39;var2&#39; &lt;&gt; Output : variable &#39;var2&#39; : Input : variable &#39;var3&#39; &lt;&gt; Output : variable &#39;var3&#39; : Input : variable &#39;var4&#39; &lt;&gt; Output : variable &#39;var4&#39; Fisher : [dataset] : Create Transformation &#34;G&#34; with events from all classes. : : Transformation, Variable selection : : Input : variable &#39;var1&#39; &lt;&gt; Output : variable &#39;var1&#39; : Input : variable &#39;var2&#39; &lt;&gt; Output : variable &#39;var2&#39; : Input : variable &#39;var3&#39; &lt;&gt; Output : variable &#39;var3&#39; : Input : variable &#39;var4&#39; &lt;&gt; Output : variable &#39;var4&#39; Factory : Booking method: PyTorch : PyTorch : [dataset] : Create Transformation &#34;D&#34; with events from all classes. : : Transformation, Variable selection : : Input : variable &#39;var1&#39; &lt;&gt; Output : variable &#39;var1&#39; : Input : variable &#39;var2&#39; &lt;&gt; Output : variable &#39;var2&#39; : Input : variable &#39;var3&#39; &lt;&gt; Output : variable &#39;var3&#39; : Input : variable &#39;var4&#39; &lt;&gt; Output : variable &#39;var4&#39; PyTorch : [dataset] : Create Transformation &#34;G&#34; with events from all classes. : : Transformation, Variable selection : : Input : variable &#39;var1&#39; &lt;&gt; Output : variable &#39;var1&#39; : Input : variable &#39;var2&#39; &lt;&gt; Output : variable &#39;var2&#39; : Input : variable &#39;var3&#39; &lt;&gt; Output : variable &#39;var3&#39; : Input : variable &#39;var4&#39; &lt;&gt; Output : variable &#39;var4&#39; : Using PyTorch - setting special configuration options : Using PyTorch version 1 : Setup PyTorch Model : Loaded pytorch train function: : Loaded pytorch optimizer: : Loaded pytorch loss function: : Loaded pytorch predict function: : Load model from file: model.pt . Training . Call the TrainAllMethods from the factory object to initiate training. Note: The output of this cell has been hidden for better readability and to avoid the verbosity. . factory.TrainAllMethods() . RecursiveScriptModule( original_name=Sequential (linear_1): RecursiveScriptModule(original_name=Linear) (relu): RecursiveScriptModule(original_name=ReLU) (linear_2): RecursiveScriptModule(original_name=Linear) (softmax): RecursiveScriptModule(original_name=Softmax) ) [Epoch 1, 64] train loss:0.224 [Epoch 1, 128] train loss:0.197 [Epoch 1, 192] train loss:0.179 [Epoch 1] val loss: 0.172 [Epoch 2, 64] train loss:0.168 [Epoch 2, 128] train loss:0.164 [Epoch 2, 192] train loss:0.155 [Epoch 2] val loss: 0.154 [Epoch 3, 64] train loss:0.151 [Epoch 3, 128] train loss:0.152 [Epoch 3, 192] train loss:0.144 [Epoch 3] val loss: 0.145 [Epoch 4, 64] train loss:0.142 [Epoch 4, 128] train loss:0.146 [Epoch 4, 192] train loss:0.137 [Epoch 4] val loss: 0.140 [Epoch 5, 64] train loss:0.136 [Epoch 5, 128] train loss:0.142 [Epoch 5, 192] train loss:0.132 [Epoch 5] val loss: 0.136 [Epoch 6, 64] train loss:0.132 [Epoch 6, 128] train loss:0.138 [Epoch 6, 192] train loss:0.129 [Epoch 6] val loss: 0.133 [Epoch 7, 64] train loss:0.129 [Epoch 7, 128] train loss:0.136 [Epoch 7, 192] train loss:0.126 [Epoch 7] val loss: 0.131 [Epoch 8, 64] train loss:0.127 [Epoch 8, 128] train loss:0.134 [Epoch 8, 192] train loss:0.123 [Epoch 8] val loss: 0.129 [Epoch 9, 64] train loss:0.124 [Epoch 9, 128] train loss:0.132 [Epoch 9, 192] train loss:0.121 [Epoch 9] val loss: 0.127 [Epoch 10, 64] train loss:0.122 [Epoch 10, 128] train loss:0.130 [Epoch 10, 192] train loss:0.119 [Epoch 10] val loss: 0.125 [Epoch 11, 64] train loss:0.121 [Epoch 11, 128] train loss:0.128 [Epoch 11, 192] train loss:0.117 [Epoch 11] val loss: 0.124 [Epoch 12, 64] train loss:0.119 [Epoch 12, 128] train loss:0.127 [Epoch 12, 192] train loss:0.116 [Epoch 12] val loss: 0.122 [Epoch 13, 64] train loss:0.117 [Epoch 13, 128] train loss:0.125 [Epoch 13, 192] train loss:0.114 [Epoch 13] val loss: 0.121 [Epoch 14, 64] train loss:0.116 [Epoch 14, 128] train loss:0.124 [Epoch 14, 192] train loss:0.113 [Epoch 14] val loss: 0.120 [Epoch 15, 64] train loss:0.114 [Epoch 15, 128] train loss:0.123 [Epoch 15, 192] train loss:0.112 [Epoch 15] val loss: 0.119 [Epoch 16, 64] train loss:0.113 [Epoch 16, 128] train loss:0.122 [Epoch 16, 192] train loss:0.111 [Epoch 16] val loss: 0.118 [Epoch 17, 64] train loss:0.112 [Epoch 17, 128] train loss:0.121 [Epoch 17, 192] train loss:0.109 [Epoch 17] val loss: 0.117 [Epoch 18, 64] train loss:0.111 [Epoch 18, 128] train loss:0.120 [Epoch 18, 192] train loss:0.108 [Epoch 18] val loss: 0.116 [Epoch 19, 64] train loss:0.110 [Epoch 19, 128] train loss:0.119 [Epoch 19, 192] train loss:0.108 [Epoch 19] val loss: 0.115 [Epoch 20, 64] train loss:0.109 [Epoch 20, 128] train loss:0.118 [Epoch 20, 192] train loss:0.107 [Epoch 20] val loss: 0.114 Finished Training on 20 Epochs! Factory : Train all methods : Building event vectors for type 2 Signal : Dataset[dataset] : create input formulas for tree TreeS : Building event vectors for type 2 Background : Dataset[dataset] : create input formulas for tree TreeB DataSetFactory : [dataset] : Number of events in input trees : : : Number of training and testing events : : Signal -- training events : 4000 : Signal -- testing events : 2000 : Signal -- training and testing events: 6000 : Background -- training events : 4000 : Background -- testing events : 2000 : Background -- training and testing events: 6000 : DataSetInfo : Correlation matrix (Signal): : - : var1 var2 var3 var4 : var1: +1.000 +0.391 +0.590 +0.813 : var2: +0.391 +1.000 +0.692 +0.734 : var3: +0.590 +0.692 +1.000 +0.851 : var4: +0.813 +0.734 +0.851 +1.000 : - DataSetInfo : Correlation matrix (Background): : - : var1 var2 var3 var4 : var1: +1.000 +0.855 +0.914 +0.965 : var2: +0.855 +1.000 +0.927 +0.936 : var3: +0.914 +0.927 +1.000 +0.970 : var4: +0.965 +0.936 +0.970 +1.000 : - DataSetFactory : [dataset] : : Factory : [dataset] : Create Transformation &#34;D&#34; with events from all classes. : : Transformation, Variable selection : : Input : variable &#39;var1&#39; &lt;&gt; Output : variable &#39;var1&#39; : Input : variable &#39;var2&#39; &lt;&gt; Output : variable &#39;var2&#39; : Input : variable &#39;var3&#39; &lt;&gt; Output : variable &#39;var3&#39; : Input : variable &#39;var4&#39; &lt;&gt; Output : variable &#39;var4&#39; Factory : [dataset] : Create Transformation &#34;G&#34; with events from all classes. : : Transformation, Variable selection : : Input : variable &#39;var1&#39; &lt;&gt; Output : variable &#39;var1&#39; : Input : variable &#39;var2&#39; &lt;&gt; Output : variable &#39;var2&#39; : Input : variable &#39;var3&#39; &lt;&gt; Output : variable &#39;var3&#39; : Input : variable &#39;var4&#39; &lt;&gt; Output : variable &#39;var4&#39; : Preparing the Decorrelation transformation... : Preparing the Gaussian transformation... TFHandler_Factory : Variable Mean RMS [ Min Max ] : -- : var1: 0.0084120 1.0019 [ -3.1195 5.7307 ] : var2: 0.0078511 0.99981 [ -3.1195 5.7307 ] : var3: 0.0083128 1.0011 [ -3.1195 5.7307 ] : var4: 0.0076997 0.99886 [ -3.1195 5.7307 ] : -- Factory : Train method: Fisher for Classification : : Preparing the Decorrelation transformation... : Preparing the Gaussian transformation... TFHandler_Fisher : Variable Mean RMS [ Min Max ] : -- : var1: 0.0084120 1.0019 [ -3.1195 5.7307 ] : var2: 0.0078511 0.99981 [ -3.1195 5.7307 ] : var3: 0.0083128 1.0011 [ -3.1195 5.7307 ] : var4: 0.0076997 0.99886 [ -3.1195 5.7307 ] : -- Fisher : Results for Fisher coefficients: : NOTE: The coefficients must be applied to TRANFORMED variables : List of the transformation: : -- Deco : -- Gauss : -- : Variable: Coefficient: : -- : var1: -0.221 : var2: -0.055 : var3: +0.032 : var4: +0.474 : (offset): -0.002 : -- : Elapsed time for training with 8000 events: 0.0486 sec Fisher : [dataset] : Evaluation of Fisher on training sample (8000 events) : Elapsed time for evaluation of 8000 events: 0.0246 sec : Creating xml weight file: dataset/weights/TMVAClassification_Fisher.weights.xml : Creating standalone class: dataset/weights/TMVAClassification_Fisher.class.C Factory : Training finished : Factory : Train method: PyTorch for Classification : : : ================================================================ : H e l p f o r M V A m e t h o d [ PyTorch ] : : : PyTorch is a scientific computing package supporting : automatic differentiation. This method wraps the training : and predictions steps of the PyTorch Python package for : TMVA, so that dataloading, preprocessing and evaluation : can be done within the TMVA system. To use this PyTorch : interface, you need to generatea model with PyTorch first. : Then, this model can be loaded and trained in TMVA. : : : &lt;Suppress this message by specifying &#34;!H&#34; in the booking option&gt; : ================================================================ : : Preparing the Decorrelation transformation... : Preparing the Gaussian transformation... TFHandler_PyTorch : Variable Mean RMS [ Min Max ] : -- : var1: 0.0084120 1.0019 [ -3.1195 5.7307 ] : var2: 0.0078511 0.99981 [ -3.1195 5.7307 ] : var3: 0.0083128 1.0011 [ -3.1195 5.7307 ] : var4: 0.0076997 0.99886 [ -3.1195 5.7307 ] : -- TFHandler_PyTorch : Variable Mean RMS [ Min Max ] : -- : var1: 0.0084120 1.0019 [ -3.1195 5.7307 ] : var2: 0.0078511 0.99981 [ -3.1195 5.7307 ] : var3: 0.0083128 1.0011 [ -3.1195 5.7307 ] : var4: 0.0076997 0.99886 [ -3.1195 5.7307 ] : -- : Split TMVA training data in 6400 training events and 1600 validation events : Print Training Model Architecture : Option SaveBestOnly: Only model weights with smallest validation loss will be stored : Elapsed time for training with 8000 events: 4.91 sec PyTorch : [dataset] : Evaluation of PyTorch on training sample (8000 events) : Elapsed time for evaluation of 8000 events: 0.0518 sec : Creating xml weight file: dataset/weights/TMVAClassification_PyTorch.weights.xml : Creating standalone class: dataset/weights/TMVAClassification_PyTorch.class.C Factory : Training finished : : Ranking input variables (method specific)... Fisher : Ranking result (top variable is best ranked) : - : Rank : Variable : Discr. power : - : 1 : var4 : 1.956e-01 : 2 : var1 : 4.019e-02 : 3 : var2 : 3.086e-03 : 4 : var3 : 2.351e-04 : - : No variable ranking supplied by classifier: PyTorch Factory : === Destroy and recreate all methods via weight files for testing === : : Reading weight file: dataset/weights/TMVAClassification_Fisher.weights.xml : Reading weight file: dataset/weights/TMVAClassification_PyTorch.weights.xml . 0%, time left: unknown 7%, time left: 0 sec 13%, time left: 0 sec 19%, time left: 0 sec 25%, time left: 0 sec 32%, time left: 0 sec 38%, time left: 0 sec 44%, time left: 0 sec 50%, time left: 0 sec 57%, time left: 0 sec 63%, time left: 0 sec 69%, time left: 0 sec 75%, time left: 0 sec 82%, time left: 0 sec 88%, time left: 0 sec 94%, time left: 0 sec . Testing . Call the TestAllMethods from the factory object to initiate testing. Note: The output of this cell has been hidden for better readability and to avoid the verbosity. . factory.TestAllMethods() . custom objects for loading model : {&#39;optimizer&#39;: &lt;class &#39;torch.optim.sgd.SGD&#39;&gt;, &#39;criterion&#39;: MSELoss(), &#39;train_func&#39;: &lt;function train at 0x1307b99e0&gt;, &#39;predict_func&#39;: &lt;function predict at 0x1307b9f80&gt;} Factory : Test all methods Factory : Test method: Fisher for Classification performance : Fisher : [dataset] : Evaluation of Fisher on testing sample (4000 events) : Elapsed time for evaluation of 4000 events: 0.016 sec Factory : Test method: PyTorch for Classification performance : : Setup PyTorch Model : Loaded pytorch train function: : Loaded pytorch optimizer: : Loaded pytorch loss function: : Loaded pytorch predict function: : Load model from file: dataset/weights/TrainedModel_PyTorch.pt PyTorch : [dataset] : Evaluation of PyTorch on testing sample (4000 events) : Elapsed time for evaluation of 4000 events: 0.0258 sec . 0%, time left: unknown 7%, time left: 0 sec 13%, time left: 0 sec 19%, time left: 0 sec 25%, time left: 0 sec 32%, time left: 0 sec 38%, time left: 0 sec 44%, time left: 0 sec 50%, time left: 0 sec 57%, time left: 0 sec 63%, time left: 0 sec 69%, time left: 0 sec 75%, time left: 0 sec 82%, time left: 0 sec 88%, time left: 0 sec 94%, time left: 0 sec . Evaluation . Call the EvaluateAllMethods from the factory object to initiate evaluation. Here we evaluate all methods and compare their performances, computing efficiencies, ROC curves, etc. using both training and tetsing data sets. . factory.EvaluateAllMethods() . Factory : Evaluate all methods Factory : Evaluate classifier: Fisher : TFHandler_Fisher : Variable Mean RMS [ Min Max ] : -- : var1: 0.015907 0.98605 [ -2.8645 3.5534 ] : var2: 0.0046941 1.0002 [ -3.2858 3.3764 ] : var3: 0.017030 0.99729 [ -2.9032 5.7307 ] : var4: -0.0011286 0.98455 [ -2.8276 3.2696 ] : -- Fisher : [dataset] : Loop over test events and fill histograms with classifier response... : TFHandler_Fisher : Variable Mean RMS [ Min Max ] : -- : var1: 0.015907 0.98605 [ -2.8645 3.5534 ] : var2: 0.0046941 1.0002 [ -3.2858 3.3764 ] : var3: 0.017030 0.99729 [ -2.9032 5.7307 ] : var4: -0.0011286 0.98455 [ -2.8276 3.2696 ] : -- Factory : Evaluate classifier: PyTorch : TFHandler_PyTorch : Variable Mean RMS [ Min Max ] : -- : var1: 0.015907 0.98605 [ -2.8645 3.5534 ] : var2: 0.0046941 1.0002 [ -3.2858 3.3764 ] : var3: 0.017030 0.99729 [ -2.9032 5.7307 ] : var4: -0.0011286 0.98455 [ -2.8276 3.2696 ] : -- PyTorch : [dataset] : Loop over test events and fill histograms with classifier response... : TFHandler_PyTorch : Variable Mean RMS [ Min Max ] : -- : var1: 0.015907 0.98605 [ -2.8645 3.5534 ] : var2: 0.0046941 1.0002 [ -3.2858 3.3764 ] : var3: 0.017030 0.99729 [ -2.9032 5.7307 ] : var4: -0.0011286 0.98455 [ -2.8276 3.2696 ] : -- : : Evaluation results ranked by best signal efficiency and purity (area) : - : DataSet MVA : Name: Method: ROC-integ : dataset PyTorch : 0.924 : dataset Fisher : 0.880 : - : : Testing efficiency compared to training efficiency (overtraining check) : - : DataSet MVA Signal efficiency: from test sample (from training sample) : Name: Method: @B=0.01 @B=0.10 @B=0.30 : - : dataset PyTorch : 0.317 (0.332) 0.757 (0.761) 0.946 (0.944) : dataset Fisher : 0.192 (0.190) 0.645 (0.646) 0.884 (0.876) : - : Dataset:dataset : Created tree &#39;TestTree&#39; with 4000 events : Dataset:dataset : Created tree &#39;TrainTree&#39; with 8000 events : Factory : Thank you for using TMVA! : For citation information, please visit: http://tmva.sf.net/citeTMVA.html . Plots . We can now analyze the results using histogram plots and ROC curves. Several histograms are produced after calling evaluation, which can be examined with the TMVA GUI or directly using the output file. . roc = factory.GetROCCurve(dataloader) roc.Draw() . . With this I&#39;ve wrapped up my project. Stay tuned for my final post about my GSoC journey at CERN. . Feel free to ask questions below in the comments or on the root forum. . . Until next time, . Anirudh Dagar .",
            "url": "https://anirudhdagar.github.io/gsoc/tmva/pytorch/root/2020/08/21/TMVA-PyTorch-Tutorial.html",
            "relUrl": "/tmva/pytorch/root/2020/08/21/TMVA-PyTorch-Tutorial.html",
            "date": " ‚Ä¢ Aug 21, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
        ,"post9": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://anirudhdagar.github.io/gsoc/jupyter/2020/02/20/test/",
            "relUrl": "/jupyter/2020/02/20/test/",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am Anirudh Dagar, final year undergraduate student at Indian Institute of Technology, Roorkee. I am passionate about open source and Machine Learning research. I am an active member at Data Science Group, IIT Roorkee. . In the past, I have worked on projects in Graph Representation Learning, Natural Language Processing and Deep Learning. Previously, I worked at MALL Lab under the mentorship of Prof. Partha Talukdar. . . . Feel free to contact me through mail for any queries. Happy to reply! :) . Follow @anirudhdagar",
          "url": "https://anirudhdagar.github.io/gsoc/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page8": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://anirudhdagar.github.io/gsoc/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}